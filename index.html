<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Imbalanced Learning Papers</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <style type="text/css">
        body { margin: 1em 5em; }
        .tag { background-color: #eee; padding: 8px; border-radius: 3px; cursor: pointer; margin-right: 1em; line-height: 300%; white-space:nowrap; }
        td { vertical-align: middle !important; }
    </style>
</head>
<body>

<h1>Imbalanced Learning Papers</h1>

<div class="row">

<div class="col-md-4">

	<h2>Filter...</h2>

	<h3>By keyword</h3>
	<input id="filter-input" placeholder="Filter papers by keyword or tag..." class='form-control' type="search">

	<h3>By topic</h3>
	<div id="tag-bar"></div>

</div>

<div class="col-md-8">
<table class="table">
<!--PAPERS-TABLE-->
	<tbody id="paper-list">
		<tr>
			<td><a target="_blank" href="http://www.aaai.org/Papers/Workshops/2000/WS-00-05/WS00-05-003.pdf">Learning from imbalanced data sets: a comparison of various strategies (Japkowicz, 2000)</a></td>
			<td><a class="tag">classification</a> <a class="tag">synthetic-data</a> <a class="tag">subsampling</a> <a class="tag">recognition</a> <a class="tag">has-summary</a></td>
			<td><a href="#" title="  - Contributions:
    - Looks at basic data characterization/factors on difficulty
    - Compares methods (random under-sampling, random over-sampling, boundary-focused resampling, basic recognition) on simple synthetic data.
  - Data factors/characterization:
    - Data generated synthetically with 'backbone' model; essentially alternating classes on the number line, where their definition of 'complexity' is related to the number of alternations over the given interval.
    - Important to note very complex data gives poor accuracy even for non-imbalanced data, so important to make any comparisons with this in mind.
    - Linearly separable data not sensitive to any degree of imbalance, regardless of training set size.
    - More complex data more sensitive to imbalance; increasing complexity correlated with increased sensitivity to imbalance.
    - Suggests that we should concentrate both on (reducing) complexity and re-balancing the data as the most important aspects of the problem.
  - Methods:
    - Purely random over- and under-sampling found to work reasonably well, especially as higher complexities.
    - Boundary-focused resampling (over- and under-) found to work about as well as purely random; no suggestion that these perform notably better.
    - Down-sampling appears to work better than over-sampling for large domains.
    - Recognition-based approach doesn't work as well until the data is at the highest complexities.
    - Recognition-based expected to perform better when there are very few minority class examples.
    - Recognition-based methods that learn the majority class work **much** better than those which learn the minority class.
  - Evaluation & Future Work
    - Only tests on *very* simple synthetic data.  More complex data might show different trends/results.  In particular:
      - Data are 1-dimensional
      - Adhere to simple 'backbone' model
      - 'Balanced imbalance' only; no sub-clustering of the classes with different characteristics.
    - Only makes use of simple feed-forward networks.
    - Only tests simple methods.
  ">Hover for summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.jair.org/index.php/jair/article/view/10302/24590">SMOTE: Synthetic Minority Over-sampling Technique (Chawla et al., 2002)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://www.jmlr.org/papers/volume8/owen07a/owen07a.pdf">Infinitely Imbalanced Logistic Regression (Owen, 2006)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf">Handling imbalanced datasets: A review (Kotsiantis et al., 2006)</a></td>
			<td><a class="tag">classification</a> <a class="tag">review</a> <a class="tag">has-summary</a></td>
			<td><a href="#" title="  - Contributions:
    - Provides a review of methods for imbalanced classification to date.
  - General:
    - Distinguishes between data-level and algorithm-level methods.
    - Data-level methods here focus on various resampling approaches: random, directed.
    - Algo-level: adjusting the costs of the various classes so as to counter the class imbalance, adjusting the decision threshold, and recognition-based (i.e., learning from one class) rather than discrimination-based (two class) learning.  Also mentions mixture-of-experts, which combines the results of more than one classifier.
    - Many learning methods prefer majority class.
  - Data-level
    - Random undersampling discards potentially useful data.
    - &quot;Another problem with this approach is that the purpose of machine learning is for the classifier to estimate the probability distribution of the target population. Since that distribution is unknown we try to estimate the population distribution using a sample distribution. Statistics tells us that as long as the sample is drawn randomly, the sample distribution can be used to estimate the population dis- tribution from where it was drawn. Hence, by learning the sample distribution we can learn to approximate the target distribution. Once we perform undersampling of the majority class, however, the sample can no longer be considered random.&quot;
    - Tomek links - &quot;can be used as an under-sampling method or as a data cleaning method. As an under-sampling method, only examples belonging to the majority class are eliminated, and as a data cleaning method, examples of both classes are removed.&quot;
  ">Hover for summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/5128907/">Learning from imbalanced data (He et al., 2009)</a></td>
			<td><a class="tag">review</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/1cdf/7eab753db92c37a29980c0cd2c46130b271e.pdf">Calibration of Machine Learning Models (Bella et al., 2010)</a></td>
			<td><a class="tag">calibration</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.semanticscholar.org/paper/Class-Imbalance%2C-Redux-Wallace-Small/a8ef5a810099178b70d1490a4e6fc4426b642cde">Class Imbalance, Redux (Wallace et al., 2011)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3238139/">Zero-inflated and Hurdle Models of Count Data with Extra Zeros: Examples from an HIV-Risk Reduction Intervention Trial (Hu et al., 2011)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Andrew_Wong8/publication/263913891_Classification_of_imbalanced_data_a_review/links/550e28780cf212874167e2af/Classification-of-imbalanced-data-a-review.pdf?origin=publication_detail">Class Imbalance, Redux (Wong et al., 2011)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://cloud.politala.ac.id/politala/Jurnal/JurnalTI/Jurnal%2035/2Fs10618-012-0295-5.pdf">Training and assessing classification rules with imbalanced data (Menardi et al., 2012)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://aircconline.com/ijdkp/V3N4/3413ijdkp02.pdf">Imbalanced Data Learning Approaches Review (Bekkar et al., 2013)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://lib.ugent.be/fulltxt/RUG01/002/163/708/RUG01-002163708_2014_0001_AC.pdf">A comparison of different methods for modelling rare events data (Paal, 2013)</a></td>
			<td><a class="tag">review</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/ca9e/f070d2a424b344b814de1196520da2f34ad7.pdf">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics (LÃ³pez et al., 2013)</a></td>
			<td><a class="tag">review</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://people.csail.mit.edu/romer/papers/TISTRespPredAds.pdf">Simple and Scalable Response Prediction for Display Advertising (Chapelle et al., 2014)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Andrea_Dal_Pozzolo/publication/283349138_Calibrating_Probability_with_Undersampling_for_Unbalanced_Classification/links/563606c308ae88cf81bcd9f1/Calibrating-Probability-with-Undersampling-for-Unbalanced-Classification.pdf">Calibrating Probability with Undersampling for Unbalanced Classification (Keren et al., 2015)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">classification</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1505.01658">A Survey of Predictive Modelling under Imbalanced Distributions (Branco1 et al., 2015)</a></td>
			<td><a class="tag">review</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.semanticscholar.org/paper/Training-deep-neural-networks-on-imbalanced-data-Wang-Liu/a0d86c44f2843a483dfffbfc03dda230bbaad4cc">Training deep neural networks on imbalanced data sets (Wang et al., 2016)</a></td>
			<td><a class="tag">deep-learning</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://link.springer.com/article/10.1007/s13748-016-0094-0">Learning from imbalanced data: open challenges and future directions (Krawczyk1, 2016)</a></td>
			<td><a class="tag">review</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Akila_Somasundaram/publication/320895027_Modelling_a_Stable_Classifier_for_Handling_Large_Scale_Data_with_Noise_and_Imbalance/links/5a0180654585152c9daf7a98/Modelling-a-Stable-Classifier-for-Handling-Large-Scale-Data-with-Noise-and-Imbalance.pdf">Modelling a Stable Classifier for Handling Large Scale Data with Noise and Imbalance (Somasundaram et al., 2017)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1710.05381">A systematic study of the class imbalance problem in convolutional neural networks (Buda1 et al., 2017)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://proceedings.mlr.press/v74/branco17a/branco17a.pdf">SMOGN: a Pre-processing Approach for Imbalanced Regression (Branco et al., 2017)</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1706.04599">On Calibration of Modern Neural Networks (Guo et al., 2017)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">neural-networks</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1803.09546">Calibrated Prediction Intervals for Neural Network Regressors (Keren et al., 2018)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">neural-networks</a> <a class="tag">regression</a></td>
			<td></td>
		</tr>
	</tbody>

<!--/PAPERS-TABLE-->
</table>

</div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
<script type="text/javascript">
// 'Random' number generator
var seed = 123;
function random() {
    var x = Math.sin(seed++) * 10000;
    return x - Math.floor(x);
}

$(function() {

    var tag_colors = {};

    // Process tags
    $("#paper-list").find('.tag').each(function() {
        var tag = $(this).text();
        if (!tag_colors.hasOwnProperty(tag)) {
            // Tag not already processed
            tag_colors[tag] = "hsl(" + 360 * random() + ',' + (25 + 70 * random()) + '%,' + (85 + 10 * random()) + '%)';
            $('#tag-bar').append('<a class="tag" style="background-color: ' + tag_colors[tag] + '">' + $(this).text() + '</a><br />')
        }
        $(this).css('background-color', tag_colors[tag])
    });

    $(".tag").click(function () {
       $("#filter-input").val($(this).text()).trigger('keyup');
    });

    $("#filter-input").keyup(function(){

        var query = $(this).val().split(" ");

        // Create a jquery object of the rows
        var rows = $("#paper-list").find("tr");

        // Show all rows if query is empty
        if ($(this).val() === "") {
            rows.show();
            return;
        }

        // Default to hide all
        rows.hide();

        // Recursively filter rows
        rows.filter(function (i, v) {
            for (var d = 0; d < query.length; ++d) {
                if (!$(this).is(":contains('" + query[d] + "')")) {
                    return false;
                }
            }
            return true;
        }).show();  // show the rows that match.

    });
});

</script>
</body>
</html>

