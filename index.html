<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Imbalanced Learning Papers</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <style type="text/css">
        body { margin: 1em 5em; }
        .tag { background-color: #eee; padding: 8px; border-radius: 3px; cursor: pointer; margin-right: 1em; line-height: 300%; white-space:nowrap; }
        td { vertical-align: middle !important; }
    </style>
</head>
<body>

<h1>Imbalanced Learning Papers</h1>

<hr />

<div class="row">

<div class="col-md-3">

	<h4>Filter by keyword</h4>
	<input id="filter-input" placeholder="Filter papers by keyword or tag..." class='form-control' type="search">
	
	<br />
	<h4>Filter by topic</h4>
	<div id="tag-bar"></div>

</div>

<div class="col-md-9">
<table class="table">
<!--PAPERS-TABLE-->
	<tbody id="paper-list">
		<tr>
			<td><a target="_blank" href="http://www.aaai.org/Papers/Workshops/2000/WS-00-05/WS00-05-003.pdf"title="  - Abstract:
    - Looks at basic data characterization/factors on difficulty
    - Compares methods (random under-sampling, random over-sampling, boundary-focused resampling, basic recognition) on simple synthetic data.
  - Introduction:
    - Class imbalance is important; justification inc. real-world examples.
    - Literature is sparse and not unified.  No comparison of suggested methods yet.
    - This paper: addresses this; characterizes difficulty in the original dataset; compares some methods.
  - Data characterization:
    - Data generated synthetically with 'backbone' model; essentially alternating classes on the number line, where their definition of 'complexity' is related to the number of alternations over the given interval.
    - Important to note very complex data gives poor accuracy even for non-imbalanced data, so important to make any comparisons with this in mind.
    - Linearly separable data not sensitive to any degree of imbalance, regardless of training set size.
    - More complex data more sensitive to imbalance; increasing complexity correlated with increased sensitivity to imbalance.
    - Suggests that we should concentrate both on (reducing) complexity and re-balancing the data as the most important aspects of the problem.
  - Comparison of methods:
    - Purely random over- and under-sampling found to work reasonably well, especially as higher complexities.
    - Boundary-focused resampling (over- and under-) found to work about as well as purely random; no suggestion that these perform notably better.
    - Down-sampling appears to work better than over-sampling for large domains.
    - Recognition-based approach doesn't work as well until the data is at the highest complexities.
    - Recognition-based expected to perform better when there are very few minority class examples.
    - Recognition-based methods that learn the majority class work **much** better than those which learn the minority class.
  - Evaluation and future work:
    - Only tests on *very* simple synthetic data.  More complex data might show different trends/results.  In particular, data in this paper:
      - Are 1-dimensional
      - Adhere to simple 'backbone' model
      - 'Balanced imbalance' only; no sub-clustering of the classes with different characteristics.
    - Only makes use of simple feed-forward networks.
    - Only tests simple methods.
  ">Learning from imbalanced data sets: a comparison of various strategies (Japkowicz, 2000)</a></td>
			<td><a class="tag">classification</a> <a class="tag">synthetic-data</a> <a class="tag">subsampling</a> <a class="tag">recognition</a> <a class="tag">has-summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.jair.org/index.php/jair/article/view/10302/24590"title="">SMOTE: Synthetic Minority Over-sampling Technique (Chawla et al., 2002)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://www.jmlr.org/papers/volume8/owen07a/owen07a.pdf"title="">Infinitely Imbalanced Logistic Regression (Owen, 2006)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf"title="  - Abstract:
    - Provides a review of some common methods for imbalanced classification to date.
  - Introduction:
    - Imbalance is important; justification and real-world examples.
    - Taxonomy between data-level and algorithm-level methods.
    - Data-level includes: forms of resampling (random  oversampling/undersampling, directed oversampling/undersampling, informed variants of these, and combinations)
    - Algo-level: adjusting the costs of the various classes so as to counter the class imbalance, adjusting the decision threshold, and recognition-based (i.e., learning from one class) rather than discrimination-based (two class) learning.  Also mentions mixture-of-experts, which combines the results of more than one classifier.
    - Mixture of experts-- combine the results of many classifiers; each usually induced after over-sampling or under-sampling the data with different over/under-sampling rates.
    - Mention of small disjuncts but no establishing description.  Notes importance of only retaining meaningful small disjuncts.
    - Inductive bias: many systems will favor the more common class [referring to the standard problem of imbalanced classes]
    - This paper does not focus on data factors [instead cites Weiss], but on techniques/solutions.
  - Data-level methods:
    - Undersampling:
      - Random elimination of majority class examples.
      - Discards potentially useful data.
      - Mismatching input/output distributions problem exists.
      - Tomek links: basic description; can be used as undersampling (remove majority only) or as data cleaning (remove examples of both classes)
    - Oversampling:
      - Random replication of minority class examples.
      - Authors suggest oversampling increases likelihood of overfitting [cites chawla2002smote and kubat1997addressing]
      - Problematic computational expense for already large datasets
      - Brief description of SMOTE process; how it avoids overfitting.
    - Feature selection:
      - Existing feature selection methods are not very useful for imbalanced data [cites zheng2004feature].  Proposes selecting features for positive and negative classes separately and then combining them.
  - Algorithm-level methods:
    - Threshold method:
      - Naive Bayes and some neural classifiers yield a score that represents the degree to which an example is a member of a class.
      - Such ranking  can  be  used  to  produce  several  classifiers,  by  varying  the  threshold  of  an example pertaining to a class
    - One-class learning:
      - One-class learning useful on extremely imbalanced data with high-dimensional noisy feature space [cites raskutti2004extreme]
    - Cost-sensitive learning:
      - Instead of changing class distribution, we can alternatively incorporate costs in decision making by defining fixed and unequal misclassification costs between classes.
      - The cost model takes the form of a cost matrix, which must be defined beforehand.
      - Brief description of how this is used in practice.
  - Combining methods:
    - Mixture of expers approaches combine the results of many classifiers.
    - This approach recognizes the fact that it is still unclear which sampling method performs best, what sampling rate should be usedâ€”and that the proper choice is probably domain  specific. 
    -Results indicate that the mixture-of experts approach performs well, generally outperforming another method (AdaBoost), and doing especially well at covering the rare, positive, examples. 
    - Examples of ensembles (SVM-based).
    - Boosting algorithms: what they do; examples inc. AdaBost, RareBoost, SMOTEBoost
  - Evaluation metrics:
    - Precision-recall issues, why accuracy is less useful.
    - Criteria for evaluating classifier performance on imbalanced data: min cost, max geometry mean, max sum, ROC.  Descriptions of each.
  - Other problems:
    - Class imbalances are not the only problem to contend with: the distribution of the data within each class is also relevant (between-class versus within-class imbalance) [cites [17], [34]]
    - Summary of other potential factors in the data -- overlapping, small disjuncts, etc.
  - Conclusions:
    - Often reported that cost-sensitive learning outperforms resampling methods
    - But clever resampling and combination methods can do quite more than cost-sensitive learning, since they can provide new info or eliminate redundant info for the learning algo.
    - Other data factors matter, but for larger datasets their effects are reduced, since the minority class is better represented.
  ">Handling imbalanced datasets: A review (Kotsiantis et al., 2006)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a> <a class="tag">undersampling</a> <a class="tag">oversampling</a> <a class="tag">mixture-of-experts</a> <a class="tag">cost-sensitive-learning</a> <a class="tag">boosting</a> <a class="tag">data-factors</a> <a class="tag">has-summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/5128907/"title="  - Abstract:
    - A comprehensive review of the development of research in learning from imbalanced data.
    - Includes: nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance.
    - Also highlights the major opportunities and challenges, as well as potential important research directions.
  - Introduction:
    - Prevalence and importance of imbalanced data.
    - Informal characterization of the problem.
    - Imbalanced data as a field is growing rapidly.
  - Nature of the problem:
    - Informal definition of between-class imbalance.
    - Between-class imblances can be binary or multiclass, though this paper focuses on binary.
    - More motivation for solving the problem, examples, etc.
    - Intrinsic vs extrinsic imbalances:
      - Intrinsically imbalanced datasets are a result of the nature of the dataspace itself.
      - Extrinsicaly imbalanced datasets may be imbalanced because of variable factors like time and storage space.
    - Relative imbalance vs absolute rarity.
    - Data complexity:
      - Broad term that encompasses a lot.
      - Within-class imbalances, where subconcepts have relatively fewer examples.
      - Small disjuncts created by the classifier for these less well-represented subconcepts.
      - Can be difficult to distinguish between very poorly represented subconcepts and simple random noise.
    - Effects can combine, e.g. commonly with general imbalanced data and the small sample size problem.
  - Solutions:
    - Uses decision trees as an illustrative example.
    - Problems with decision trees (for imbalanced data): reducing examples at each partition.
    - Notational conventions.
    - Sampling methods:
      - Basic description.
      - Typically these methods show improvements for most imbalanced datasets.
      - Random oversampling and undersampling, and the relative potential pitfalls of each.
      - Informed undersampling to address information loss in random undersampling; EasyEnsemble, BalanceCascade, KNN, OSS.
      - Synthetic resampling: SMOTE, how it works, problem of over-generalization.
      - ADASYN to address the over-generalization problem in SMOTE.
      - Resampling with data cleaning: Tomek links, and combination of TL with other methods.
      - Cluster-based sampling method: CBO using K-means.
      - Integration of sampling and boosting.
    - Cost-sensitive methods:
      - Basic idea of assigning costs to misclassified examples.
      - Typically use a pre-defined cost matrix.
      - Can be superior to sampling methods in certain domains, and hence are a viable alternative.
      - Categories of methods: dataspace weighting, ensemble-based, and classifier-incorporated.
      - Cost-sensitive boosting methods.
      - Implementations in decision trees and neural networks.  Brief mention of other classifiers.
    - Kernel-based methods and active learning:
      - Kernel based methods:
        - Based on statistical learning theory (VC)
        - Integration with sampling methods
        - Kernel modification methods.
      - Active learning:
        - Traditionally used for unsupervised learning, but applied to IL
        - Selects most informative instances from unseen training data.
    - Additional methods:
      - One-class learning
      - Autoassociators
      - Etc.
  - Assessment metrics for imbalanced learning:
    - Accuracy/error rate are most frequent for classifier assessment, but have problems with IL.
    - Simple alternatives: precision, recall, F-measure, G-mean. Issues with these.
    - ROC curve to address the issues above.  Limitations of ROC curves.
    - PR to address limitations of ROC curves.
    - Cost curves for confidence intervals and inference.
    - Assessment for multiclass IL.
  - Opportunities and challenges:
    - Increasing availability of large amounts of data enriches opportunities of learning from imbalanced data.
    - Most current work looks at specific algorithms with little work on theoretical underpinnings.
    - Not clear which IL methods (if any) should be used for given data/task.
    - Argues for a generlalist approach to making ML methods.
    - Key questions:
      - 1. What kind of assumptions will make imbalanced learning algorithms work better compared to learning from the original distributions?
      - 2. To what degree should one balance the original data set?
      - 3. How do imbalanced data distributions affect the computational complexity of learning algorithms?
      - 4. What is the general error bound given an imbalanced data distribution?
      - 5. Is there a general theoretical methodology that can alleviate the impediment of learning from imbalanced data sets for specific algorithms and application domains?
    - Lack of and need for a uniform benchmark platform for imbalanced learning.
    - Lack of and need for standardized evaluation practices.
    - Need for work on incremental learning.
    - Need for work on semisupervised learning.
  ">Learning from imbalanced data (He et al., 2009)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a> <a class="tag">resampling</a> <a class="tag">informed-resampling</a> <a class="tag">smote</a> <a class="tag">adasyn</a> <a class="tag">kernel-based</a> <a class="tag">active-learning</a> <a class="tag">decision-trees</a> <a class="tag">neural-networks</a> <a class="tag">tomek-links</a> <a class="tag">assessment-metrics</a> <a class="tag">cost-sensitive</a> <a class="tag">has-summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/1cdf/7eab753db92c37a29980c0cd2c46130b271e.pdf"title="">Calibration of Machine Learning Models (Bella et al., 2010)</a></td>
			<td><a class="tag">calibration</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.semanticscholar.org/paper/Class-Imbalance%2C-Redux-Wallace-Small/a8ef5a810099178b70d1490a4e6fc4426b642cde"title="  - Abstract:
    - Looks at dataset characteristics that exacerbate the calss-imbalance problem; theoretical understanding of imbalance.
    - Advocates the approach of bagging an ensemble of classifiers induced over balanced bootstrap training samples.
  - Introduction:
    - Class imbalance problem and importance.
    - Inadequacy of standard methods
    - Many methods have been proposed, but relatively little work to characterize data factors.  Hence not clear which method should be used for any given task.
    - Summary of resampling, cost-sensitive learning, etc.
    - Resampling is a high-variance technique
    - 'Bagging' classifiers into an ensemble reduces variance of classifiers.
    - => such ensembles of classifiers induces on resampled data should always be used
    - Claims to show that &quot;cost-sensitive approaches that look to improve performance achieved under imbalance by, for example, modifying the relative costs of false negatives to false positives in an objective function, will generally be effective only when the training dataset is not separable.&quot;
    - Contributions:
      - Probabilistic theory to quantify the effects of imbalance on the induction of empirical-loss minimizing models
      - Analysis of popular methods and discussion of when they might work best.
  - Theoretical analysis of imbalance (data factors):
    - Basic definitions and characterization of the imbalanced data problem.
  - Probabilistic framework
  - Synthetic data generation and experiments:
    - Intended to back up probabilistic framework
    - Run with SMOTE and weighted-SVM
    - Test various data factors and compares results with what is expected from the framework
  - Conclusions:
    - It follows from the probabilistic interpretation of class imbalance developed in this paper that re-sampling methods, specifically undersampling, ought to be used to handle imbalance in most scenarios 
    - Bagging should be used to reduce the variance of this approach
  ">Class Imbalance, Redux (Wallace et al., 2011)</a></td>
			<td><a class="tag">classification</a> <a class="tag">data-factors</a> <a class="tag">ensembles</a> <a class="tag">synthetic-data</a> <a class="tag">smote</a> <a class="tag">svm</a> <a class="tag">has-summary</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3238139/"title="">Zero-inflated and Hurdle Models of Count Data with Extra Zeros: Examples from an HIV-Risk Reduction Intervention Trial (Hu et al., 2011)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Andrew_Wong8/publication/263913891_Classification_of_imbalanced_data_a_review/links/550e28780cf212874167e2af/Classification-of-imbalanced-data-a-review.pdf?origin=publication_detail"title="">Classification of imbalanced data: a review (Wong et al., 2011)</a></td>
			<td><a class="tag">review</a> <a class="tag">classification</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://cloud.politala.ac.id/politala/Jurnal/JurnalTI/Jurnal%2035/2Fs10618-012-0295-5.pdf"title="">Training and assessing classification rules with imbalanced data (Menardi et al., 2012)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://pdfs.semanticscholar.org/ca9e/f070d2a424b344b814de1196520da2f34ad7.pdf"title="">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics (LÃ³pez et al., 2013)</a></td>
			<td><a class="tag">review</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://people.csail.mit.edu/romer/papers/TISTRespPredAds.pdf"title="">Simple and Scalable Response Prediction for Display Advertising (Chapelle et al., 2014)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Andrea_Dal_Pozzolo/publication/283349138_Calibrating_Probability_with_Undersampling_for_Unbalanced_Classification/links/563606c308ae88cf81bcd9f1/Calibrating-Probability-with-Undersampling-for-Unbalanced-Classification.pdf"title="">Calibrating Probability with Undersampling for Unbalanced Classification (Keren et al., 2015)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">classification</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1505.01658"title="">A Survey of Predictive Modelling under Imbalanced Distributions (Branco1 et al., 2015)</a></td>
			<td><a class="tag">review</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.semanticscholar.org/paper/Training-deep-neural-networks-on-imbalanced-data-Wang-Liu/a0d86c44f2843a483dfffbfc03dda230bbaad4cc"title="">Training deep neural networks on imbalanced data sets (Wang et al., 2016)</a></td>
			<td><a class="tag">deep-learning</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://link.springer.com/article/10.1007/s13748-016-0094-0"title="">Learning from imbalanced data: open challenges and future directions (Krawczyk1, 2016)</a></td>
			<td><a class="tag">review</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://www.researchgate.net/profile/Akila_Somasundaram/publication/320895027_Modelling_a_Stable_Classifier_for_Handling_Large_Scale_Data_with_Noise_and_Imbalance/links/5a0180654585152c9daf7a98/Modelling-a-Stable-Classifier-for-Handling-Large-Scale-Data-with-Noise-and-Imbalance.pdf"title="">Modelling a Stable Classifier for Handling Large Scale Data with Noise and Imbalance (Somasundaram et al., 2017)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1710.05381"title="">A systematic study of the class imbalance problem in convolutional neural networks (Buda1 et al., 2017)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="http://proceedings.mlr.press/v74/branco17a/branco17a.pdf"title="">SMOGN: a Pre-processing Approach for Imbalanced Regression (Branco et al., 2017)</a></td>
			<td></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1706.04599"title="">On Calibration of Modern Neural Networks (Guo et al., 2017)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">neural-networks</a></td>
		</tr>
		<tr>
			<td><a target="_blank" href="https://arxiv.org/pdf/1803.09546"title="">Calibrated Prediction Intervals for Neural Network Regressors (Keren et al., 2018)</a></td>
			<td><a class="tag">calibration</a> <a class="tag">neural-networks</a> <a class="tag">regression</a></td>
		</tr>
	</tbody>

<!--/PAPERS-TABLE-->
</table>

</div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
<script type="text/javascript">
// 'Random' number generator
var seed = 123;
function random() {
    var x = Math.sin(seed++) * 10000;
    return x - Math.floor(x);
}

$(function() {

    var tag_colors = {};

    // Process tags
    $("#paper-list").find('.tag').each(function() {
        var tag = $(this).text();
        if (!tag_colors.hasOwnProperty(tag)) {
            // Tag not already processed
            tag_colors[tag] = "hsl(" + 360 * random() + ',' + (25 + 70 * random()) + '%,' + (85 + 10 * random()) + '%)';
            $('#tag-bar').append('<a class="tag" style="background-color: ' + tag_colors[tag] + '">' + $(this).text() + '</a><br />')
        }
        $(this).css('background-color', tag_colors[tag])
    });

    $(".tag").click(function () {
       $("#filter-input").val($(this).text()).trigger('keyup');
    });

    $("#filter-input").keyup(function(){

        var query = $(this).val().split(" ");

        // Create a jquery object of the rows
        var rows = $("#paper-list").find("tr");

        // Show all rows if query is empty
        if ($(this).val() === "") {
            rows.show();
            return;
        }

        // Default to hide all
        rows.hide();

        // Recursively filter rows
        rows.filter(function (i, v) {
            for (var d = 0; d < query.length; ++d) {
                if (!$(this).is(":contains('" + query[d] + "')")) {
                    return false;
                }
            }
            return true;
        }).show();  // show the rows that match.

    });
});

</script>
</body>
</html>

