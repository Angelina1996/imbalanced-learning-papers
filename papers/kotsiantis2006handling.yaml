---
title: 'Handling imbalanced datasets: A review'
year: 2006
cited_by_count: 443
authors:
  - Sotiris Kotsiantis
  - Dimitris Kanellopoulos
  - Panayiotis Pintelas
tags:
  - review
  - classification
  - undersampling
  - oversampling
  - mixture-of-experts
  - cost-sensitive-learning
  - boosting
  - data-factors
links:
  - https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf
summary: |
  - Abstract:
    - Provides a review of some common methods for imbalanced classification to date.
  - Introduction:
    - Imbalance is important; justification and real-world examples.
    - Taxonomy between data-level and algorithm-level methods.
    - Data-level includes: forms of resampling (random  oversampling/undersampling, directed oversampling/undersampling, informed variants of these, and combinations)
    - Algo-level: adjusting the costs of the various classes so as to counter the class imbalance, adjusting the decision threshold, and recognition-based (i.e., learning from one class) rather than discrimination-based (two class) learning.  Also mentions mixture-of-experts, which combines the results of more than one classifier.
    - Mixture of experts-- combine the results of many classifiers; each usually induced after over-sampling or under-sampling the data with different over/under-sampling rates.
    - Mention of small disjuncts but no establishing description.  Notes importance of only retaining meaningful small disjuncts.
    - Inductive bias: many systems will favor the more common class [referring to the standard problem of imbalanced classes]
    - This paper does not focus on data factors [instead cites Weiss], but on techniques/solutions.
  - Data-level methods:
    - Undersampling:
      - Random elimination of majority class examples.
      - Discards potentially useful data.
      - Mismatching input/output distributions problem exists.
      - Tomek links: basic description; can be used as undersampling (remove majority only) or as data cleaning (remove examples of both classes)
    - Oversampling:
      - Random replication of minority class examples.
      - Authors suggest oversampling increases likelihood of overfitting [cites chawla2002smote and kubat1997addressing]
      - Problematic computational expense for already large datasets
      - Brief description of SMOTE process; how it avoids overfitting.
    - Feature selection:
      - Existing feature selection methods are not very useful for imbalanced data [cites zheng2004feature].  Proposes selecting features for positive and negative classes separately and then combining them.
  - Algorithm-level methods:
    - Threshold method:
      - Naive Bayes and some neural classifiers yield a score that represents the degree to which an example is a member of a class.
      - Such ranking  can  be  used  to  produce  several  classifiers,  by  varying  the  threshold  of  an example pertaining to a class
    - One-class learning:
      - One-class learning useful on extremely imbalanced data with high-dimensional noisy feature space [cites raskutti2004extreme]
    - Cost-sensitive learning:
      - Instead of changing class distribution, we can alternatively incorporate costs in decision making by defining fixed and unequal misclassification costs between classes.
      - The cost model takes the form of a cost matrix, which must be defined beforehand.
      - Brief description of how this is used in practice.
  - Combining methods:
    - Mixture of expers approaches combine the results of many classifiers.
    - This approach recognizes the fact that it is still unclear which sampling method performs best, what sampling rate should be usedâ€”and that the proper choice is probably domain  specific. 
    -Results indicate that the mixture-of experts approach performs well, generally outperforming another method (AdaBoost), and doing especially well at covering the rare, positive, examples. 
    - Examples of ensembles (SVM-based).
    - Boosting algorithms: what they do; examples inc. AdaBost, RareBoost, SMOTEBoost
  - Evaluation metrics:
    - Precision-recall issues, why accuracy is less useful.
    - Criteria for evaluating classifier performance on imbalanced data: min cost, max geometry mean, max sum, ROC.  Descriptions of each.
  - Other problems:
    - Class imbalances are not the only problem to contend with: the distribution of the data within each class is also relevant (between-class versus within-class imbalance) [cites [17], [34]]
    - Summary of other potential factors in the data -- overlapping, small disjuncts, etc.
  - Conclusions:
    - Often reported that cost-sensitive learning outperforms resampling methods
    - But clever resampling and combination methods can do quite more than cost-sensitive learning, since they can provide new info or eliminate redundant info for the learning algo.
...
